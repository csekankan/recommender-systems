{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "447R2K8kRY0Z"
      },
      "source": [
        "## Imports\n",
        "\n",
        "First let's get our dependencies and imports out of the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM3uchF8deZL"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfpLMl1tc4OE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrJ3cFNLJvCI",
        "outputId": "d71405fa-a871-4701-e640-29c8ab6cc199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1kl65YOvoSAMSgszQQbua2q4Zwe1HgPct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cdd054-e7b0-4660-c56e-ef3d49ac8412",
        "id": "BbvkxkXpJvCI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kl65YOvoSAMSgszQQbua2q4Zwe1HgPct\n",
            "To: /content/data.zip\n",
            "\r  0% 0.00/4.16M [00:00<?, ?B/s]\r100% 4.16M/4.16M [00:00<00:00, 278MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o \"data.zip\"  -d  \"/content\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59905a3c-149f-42ea-ff96-c778e52acabc",
        "id": "MNm3UFwnJvCJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "  inflating: /content/amazon_min.csv  \n",
            "  inflating: /content/book_min.csv   \n",
            "  inflating: /content/ciao_min.csv   \n",
            "  inflating: /content/ecom_min.csv   \n",
            "  inflating: /content/food_min.csv   \n",
            "  inflating: /content/movies_min.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "ratings_df =pd.read_csv('amazon_min.csv')\n",
        "ratings_df=ratings_df[['userID','itemID','rating','time']]\n",
        "ratings_df.columns =[ 'userID', 'itemID', 'rating','timestamp']\n",
        "items_df=ratings_df[['itemID']]\n",
        "items_df = items_df.sort_values(by='itemID', ascending=False)\n",
        "items_df = items_df.drop_duplicates(subset='itemID', keep=\"first\")"
      ],
      "metadata": {
        "id": "nqGg0paIJvCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IFq7usGkNb4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_ids = list(set(list(ratings_df.itemID.unique())))\n",
        "user_ids = list(set(list(ratings_df.userID.unique())))\n",
        "dict_users = {}\n",
        "index = 0\n",
        "for ids in sorted(user_ids):\n",
        "    dict_users[ids] = index\n",
        "    index += 1\n",
        "dict_items = {}\n",
        "index = 0\n",
        "for ids in sorted(item_ids):\n",
        "    dict_items[ids] = index\n",
        "    index += 1\n"
      ],
      "metadata": {
        "id": "wbdwZjvLLUsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df['userID'] = ratings_df.userID.map(dict_users)\n",
        "ratings_df['itemID'] = ratings_df.itemID.map(dict_items)\n",
        "items_df['order_id'] = items_df.itemID.map(dict_items)"
      ],
      "metadata": {
        "id": "_hPUhWhMZNR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items_df['itemID']=items_df['itemID'].astype(str)"
      ],
      "metadata": {
        "id": "RGvtcXZtH-XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocessing"
      ],
      "metadata": {
        "id": "Oqb-57AF_NiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Ref:- https://www.tensorflow.org/recommenders/\"\"\"\n",
        "\n",
        "import collections\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# Permalinks to download itemlens data.\n",
        "\n",
        "RATINGS_DATA_COLUMNS = [\"userID\",'itemID', 'rating','timestamp']\n",
        "itemS_DATA_COLUMNS = [\"itemID\"]\n",
        "OUTPUT_TRAINING_DATA_FILENAME = \"train.tfrecord\"\n",
        "OUTPUT_TESTING_DATA_FILENAME = \"test.tfrecord\"\n",
        "OUTPUT_ITEM_VOCAB_FILENAME = \"item_vocab.json\"\n",
        "PAD_ITEM_ID = 0\n",
        "PAD_RATING = 0.0\n",
        "UNKNOWN_STR = \"UNK\"\n",
        "VOCAB_ITEM_ID_INDEX = 0\n",
        "VOCAB_COUNT_INDEX = 0\n",
        "\n",
        "\n",
        "class ItemInfo(\n",
        "    collections.namedtuple(\n",
        "        \"ItemInfo\", [\"item_id\", \"timestamp\", \"rating\"])):\n",
        "  \"\"\"Data holder of basic information of a item.\"\"\"\n",
        "  __slots__ = ()\n",
        "\n",
        "  def __new__(cls,\n",
        "              item_id=0,\n",
        "              timestamp=0,\n",
        "              rating=0,\n",
        "             ):\n",
        "    return super(ItemInfo, cls).__new__(cls, item_id, timestamp, rating,\n",
        "                                        )\n",
        "\n",
        "\n",
        "def convert_to_timelines(ratings_df):\n",
        "  \"\"\"Convert ratings data to user.\"\"\"\n",
        "  timelines = collections.defaultdict(list)\n",
        "  item_counts = collections.Counter()\n",
        "  for user_id, item_id, rating, timestamp in ratings_df.values:\n",
        "    timelines[user_id].append(\n",
        "        ItemInfo(item_id=item_id, timestamp=int(timestamp), rating=rating))\n",
        "    item_counts[item_id] += 1\n",
        "  # Sort per-user timeline by timestamp\n",
        "  for (user_id, context) in timelines.items():\n",
        "    context.sort(key=lambda x: x.timestamp)\n",
        "    timelines[user_id] = context\n",
        "  return timelines, item_counts\n",
        "\n",
        "\n",
        "def generate_items_dict(items_df):\n",
        "  \"\"\"Generates items dictionary from items dataframe.\"\"\"\n",
        "  item_dict = {\n",
        "      item_id[0]: ItemInfo(item_id=item_id[0])\n",
        "      for item_id in items_df.values\n",
        "  }\n",
        "  item_dict[0] = ItemInfo()\n",
        "  return item_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_examples_from_single_timeline(timeline,\n",
        "                                           items_dict,\n",
        "                                           max_context_len=100,\n",
        "                                           max_context_item_genre_len=320):\n",
        "  \"\"\"Generate TF examples from a single user timeline.\n",
        "\n",
        "  Generate TF examples from a single user timeline. Timeline with length less\n",
        "  than minimum timeline length will be skipped. And if context user history\n",
        "  length is shorter than max_context_len, features will be padded with default\n",
        "  values.\n",
        "\n",
        "  Args:\n",
        "    timeline: The timeline to generate TF examples from.\n",
        "    items_dict: Dictionary of all itemInfos.\n",
        "    max_context_len: The maximum length of the context. If the context history\n",
        "      length is less than max_context_length, features will be padded with\n",
        "      default values.\n",
        "    max_context_item_genre_len: The length of item genre feature.\n",
        "\n",
        "  Returns:\n",
        "    examples: Generated examples from this single timeline.\n",
        "  \"\"\"\n",
        "  examples = []\n",
        "  for label_idx in range(1, len(timeline)):\n",
        "    start_idx = max(0, label_idx - max_context_len)\n",
        "    context = timeline[start_idx:label_idx]\n",
        "    # Pad context with out-of-vocab item id 0.\n",
        "    while len(context) < max_context_len:\n",
        "      context.append(ItemInfo())\n",
        "    label_item_id = int(timeline[label_idx].item_id)\n",
        "    context_item_id = [int(ids.item_id) for ids in context]\n",
        "   \n",
        "    feature = {\n",
        "        \"context_item_id\":\n",
        "            tf.train.Feature(\n",
        "                int64_list=tf.train.Int64List(value=context_item_id)),\n",
        "       \n",
        "        \"label_item_id\":\n",
        "            tf.train.Feature(\n",
        "                int64_list=tf.train.Int64List(value=[label_item_id]))\n",
        "    }\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    examples.append(tf_example)\n",
        "\n",
        "  return examples\n",
        "\n",
        "\n",
        "def generate_examples_from_timelines(timelines,\n",
        "                                     items_df,\n",
        "                                     min_timeline_len=3,\n",
        "                                     max_context_len=100,\n",
        "                                     max_context_item_genre_len=320,\n",
        "                                     train_data_fraction=0.9,\n",
        "                                     random_seed=None,\n",
        "                                     shuffle=True):\n",
        "  \"\"\"Convert user timelines to tf examples.\n",
        "\n",
        "  Convert user timelines to tf examples by adding all possible context-label\n",
        "  pairs in the examples pool.\n",
        "\n",
        "  \"\"\"\n",
        "  examples = []\n",
        "  items_dict = generate_items_dict(items_df)\n",
        "  progress_bar = tf.keras.utils.Progbar(len(timelines))\n",
        "\n",
        "  for timeline in timelines.values():\n",
        "    if len(timeline) < min_timeline_len:\n",
        "      progress_bar.add(1)\n",
        "      continue\n",
        "    single_timeline_examples = generate_examples_from_single_timeline(\n",
        "        timeline=timeline,\n",
        "        items_dict=items_dict,\n",
        "        max_context_len=max_context_len,\n",
        "        max_context_item_genre_len=max_context_item_genre_len)\n",
        "    examples.extend(single_timeline_examples)\n",
        "   \n",
        "    progress_bar.add(1)\n",
        "  # Split the examples into train, test sets.\n",
        "  if shuffle:\n",
        "    random.seed(random_seed)\n",
        "    random.shuffle(examples)\n",
        "  last_train_index = round(len(examples) * train_data_fraction)\n",
        "\n",
        "  train_examples = examples[:last_train_index]\n",
        "  test_examples = examples[last_train_index:]\n",
        "  return train_examples, test_examples\n",
        "\n",
        "\n",
        "def generate_item_feature_vocabs(items_df, item_counts):\n",
        "  \"\"\"Generate vocabularies for item features.\n",
        "\n",
        "  Generate vocabularies for item features (item_id, genre, year), sorted by\n",
        "  usage count. Vocab id 0 will be reserved for default padding value.\n",
        "\n",
        "  \"\"\"\n",
        "  item_vocab = []\n",
        "\n",
        "  for item_id in items_df.values:\n",
        "    count = item_counts.get(item_id[0]) or 0\n",
        "    item_vocab.append([item_id[0], count])\n",
        "  \n",
        "  item_vocab.sort(key=lambda x: x[0], reverse=True)  # by count\n",
        "  \n",
        "  return item_vocab\n",
        "\n",
        "\n",
        "def write_tfrecords(tf_examples, filename):\n",
        "  \"\"\"Writes tf examples to tfrecord file, and returns the count.\"\"\"\n",
        "  with tf.io.TFRecordWriter(filename) as file_writer:\n",
        "    length = len(tf_examples)\n",
        "    progress_bar = tf.keras.utils.Progbar(length)\n",
        "    for example in tf_examples:\n",
        "      file_writer.write(example.SerializeToString())\n",
        "      progress_bar.add(1)\n",
        "    return length\n",
        "\n",
        "\n",
        "def write_vocab_json(vocab, filename):\n",
        "  \"\"\"Write generated item vocabulary to specified file.\"\"\"\n",
        "  with open(filename, \"w\", encoding=\"utf-8\") as jsonfile:\n",
        "    json.dump(vocab, jsonfile, indent=2)\n",
        "\n",
        "\n",
        "def write_vocab_txt(vocab, filename):\n",
        "  with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in vocab:\n",
        "      f.write(str(item) + \"\\n\")\n",
        "\n",
        "\n",
        "def generate_datasets(extracted_data_dir,\n",
        "                      output_dir,\n",
        "                      min_timeline_length,\n",
        "                      max_context_length,\n",
        "                      max_context_item_genre_length,\n",
        "                      min_rating=None,\n",
        "                      build_vocabs=True,\n",
        "                      train_data_fraction=0.9,\n",
        "                      train_filename=OUTPUT_TRAINING_DATA_FILENAME,\n",
        "                      test_filename=OUTPUT_TESTING_DATA_FILENAME,\n",
        "                      vocab_filename=OUTPUT_ITEM_VOCAB_FILENAME,\n",
        "                      ):\n",
        "  \"\"\"Generates train and test datasets as TFRecord, and returns stats.\"\"\"\n",
        "\n",
        "  logging.info(\"Generating item rating user timelines.\")\n",
        "  timelines, item_counts = convert_to_timelines(ratings_df)\n",
        "  logging.info(\"Generating train and test examples.\")\n",
        "  train_examples, test_examples = generate_examples_from_timelines(\n",
        "      timelines=timelines,\n",
        "      items_df=items_df,\n",
        "      min_timeline_len=min_timeline_length,\n",
        "      max_context_len=max_context_length,\n",
        "      max_context_item_genre_len=max_context_item_genre_length,\n",
        "      train_data_fraction=train_data_fraction)\n",
        "\n",
        "  if not tf.io.gfile.exists(output_dir):\n",
        "    tf.io.gfile.makedirs(output_dir)\n",
        "  logging.info(\"Writing generated training examples.\")\n",
        "  train_file = os.path.join(output_dir, train_filename)\n",
        "  train_size = write_tfrecords(tf_examples=train_examples, filename=train_file)\n",
        "  logging.info(\"Writing generated testing examples.\")\n",
        "  test_file = os.path.join(output_dir, test_filename)\n",
        "  test_size = write_tfrecords(tf_examples=test_examples, filename=test_file)\n",
        "  stats = {\n",
        "      \"train_size\": train_size,\n",
        "      \"test_size\": test_size,\n",
        "      \"train_file\": train_file,\n",
        "      \"test_file\": test_file,\n",
        "  }\n",
        "\n",
        "  if build_vocabs:\n",
        "    item_vocab = (\n",
        "        generate_item_feature_vocabs(\n",
        "            items_df=items_df, item_counts=item_counts))\n",
        "    vocab_file = os.path.join(output_dir, vocab_filename)\n",
        "    write_vocab_json(item_vocab, filename=vocab_file)\n",
        "    stats.update({\n",
        "        \"vocab_size\": len(item_vocab),\n",
        "        \"vocab_file\": vocab_file,\n",
        "        \"vocab_max_id\": max([arr[VOCAB_ITEM_ID_INDEX] for arr in item_vocab])\n",
        "    })\n",
        "\n",
        "    \n",
        "\n",
        "  return stats\n",
        "\n",
        "\n",
        "\n",
        "stats = generate_datasets(\n",
        "      extracted_data_dir=\"ml-1m\",\n",
        "      output_dir=\"data/processing\",\n",
        "      min_timeline_length=3,\n",
        "      max_context_length=10,\n",
        "      max_context_item_genre_length=4,\n",
        "      min_rating=0,\n",
        "      build_vocabs=True,\n",
        "      train_data_fraction=.8,\n",
        "  )\n",
        "print(stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbNqAEsmztKe",
        "outputId": "0f413181-26a3-452e-aaeb-2e42a8fb3887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2952/2952 [==============================] - 2s 644us/step\n",
            "50539/50539 [==============================] - 1s 12us/step\n",
            "12635/12635 [==============================] - 0s 11us/step\n",
            "{'train_size': 50539, 'test_size': 12635, 'train_file': 'data/processing/train.tfrecord', 'test_file': 'data/processing/test.tfrecord', 'vocab_size': 2771, 'vocab_file': 'data/processing/item_vocab.json', 'vocab_max_id': 'B00L3YHF6O'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_filename = \"./data/processing/train.tfrecord\"\n",
        "train = tf.data.TFRecordDataset(train_filename)\n",
        "\n",
        "test_filename = \"./data/processing/test.tfrecord\"\n",
        "test = tf.data.TFRecordDataset(test_filename)\n"
      ],
      "metadata": {
        "id": "dYVtG8cZThAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_description = {\n",
        "    'context_item_id': tf.io.FixedLenFeature([10], tf.int64, default_value=np.repeat(0, 10)),\n",
        "    'label_item_id': tf.io.FixedLenFeature([1], tf.int64, default_value=0),\n",
        "}\n"
      ],
      "metadata": {
        "id": "_9GT4jw5ThFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def _parse_function(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "train_ds = train.map(_parse_function).map(lambda x: {\n",
        "    \"context_item_id\": tf.strings.as_string(x[\"context_item_id\"]),\n",
        "    \"label_item_id\": tf.strings.as_string(x[\"label_item_id\"])\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "YuXhbXctXlhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for x in train_ds.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj53oWV5ZrQl",
        "outputId": "8521ec35-fb38-42ab-9b77-882a66a72024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'context_item_id': array([b'1513', b'1648', b'971', b'1671', b'1452', b'2023', b'2103',\n",
            "       b'1810', b'1744', b'2266'], dtype=object),\n",
            " 'label_item_id': array([b'2401'], dtype=object)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ2aLaCndSYP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "test_ds = test.map(_parse_function).map(lambda x: {\n",
        "    \"context_item_id\": tf.strings.as_string(x[\"context_item_id\"]),\n",
        "    \"label_item_id\": tf.strings.as_string(x[\"label_item_id\"])\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szgmv8fvc7Tj"
      },
      "outputs": [],
      "source": [
        "# movies = tfds.load(\"movielens/1m-movies\", split='train')\n",
        "# movies = movies.map(lambda x: x[\"movie_id\"])\n",
        "# movie_ids = movies.batch(1_000)\n",
        "# unique_movie_ids = np.unique(np.concatenate(list(movie_ids)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df['itemID']=ratings_df['itemID'].astype(str)\n",
        "items_dict = ratings_df[['itemID']].drop_duplicates()\n",
        "\n",
        "items= tf.data.Dataset.from_tensor_slices(dict(items_dict))\n",
        "items = items.map(lambda x: x['itemID'])\n",
        "items_list =items.batch(1_000)\n",
        "unique_item_ids = unique_items = np.unique(np.concatenate(list(items_list),axis =0))\n"
      ],
      "metadata": {
        "id": "H6KSp_hN3c9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxFmdiVxT8j_"
      },
      "source": [
        "## Implementing a sequential model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP3OS_HPeefT"
      },
      "outputs": [],
      "source": [
        "embedding_dimension = 32\n",
        "\n",
        "query_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_item_ids, mask_token=None),\n",
        "    tf.keras.layers.Embedding(len(unique_item_ids) + 1, embedding_dimension), \n",
        "    tf.keras.layers.GRU(embedding_dimension),\n",
        "])\n",
        "\n",
        "candidate_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_item_ids, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_item_ids) + 1, embedding_dimension)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ich7YzXXAWq"
      },
      "source": [
        "The metrics, task and full model are defined similar to the basic retrieval model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KDYuhG9u9cq"
      },
      "outputs": [],
      "source": [
        "metrics = tfrs.metrics.FactorizedTopK(\n",
        "  candidates=items.batch(128).map(candidate_model)\n",
        ")\n",
        "\n",
        "task = tfrs.tasks.Retrieval(\n",
        "  metrics=metrics\n",
        ")\n",
        "\n",
        "class Model(tfrs.Model):\n",
        "\n",
        "    def __init__(self, query_model, candidate_model):\n",
        "        super().__init__()\n",
        "        self._query_model = query_model\n",
        "        self._candidate_model = candidate_model\n",
        "\n",
        "        self._task = task\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "      \n",
        "        watch_history = features[\"context_item_id\"]\n",
        "        watch_next_label = features[\"label_item_id\"]\n",
        "\n",
        "        query_embedding = self._query_model(watch_history)       \n",
        "        candidate_embedding = self._candidate_model(watch_next_label)\n",
        "        \n",
        "        return self._task(query_embedding, candidate_embedding, compute_metrics=not training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j22_GTTeXa_T"
      },
      "source": [
        "## Fitting and evaluating\n",
        "\n",
        "We can now compile, train and evaluate our sequential retrieval model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c2wAqPVXYgz"
      },
      "outputs": [],
      "source": [
        "model = Model(query_model, candidate_model)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wto55LJWuj0"
      },
      "outputs": [],
      "source": [
        "cached_train = train_ds.shuffle(10_000).batch(12800).cache()\n",
        "cached_test = test_ds.batch(2560).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcXH5aDcWyJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc822da6-4443-49be-9c34-00d39bf227eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "5/5 [==============================] - 5s 488ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 117786.1562 - regularization_loss: 0.0000e+00 - total_loss: 117786.1562\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 0s 58ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 113763.7125 - regularization_loss: 0.0000e+00 - total_loss: 113763.7125\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 108690.1750 - regularization_loss: 0.0000e+00 - total_loss: 108690.1750\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 103671.2344 - regularization_loss: 0.0000e+00 - total_loss: 103671.2344\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 99958.5328 - regularization_loss: 0.0000e+00 - total_loss: 99958.5328\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 0s 51ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 96690.9891 - regularization_loss: 0.0000e+00 - total_loss: 96690.9891\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 0s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 93480.6625 - regularization_loss: 0.0000e+00 - total_loss: 93480.6625\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 90674.6766 - regularization_loss: 0.0000e+00 - total_loss: 90674.6766\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 88105.5688 - regularization_loss: 0.0000e+00 - total_loss: 88105.5688\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 86367.4672 - regularization_loss: 0.0000e+00 - total_loss: 86367.4672\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 84674.4250 - regularization_loss: 0.0000e+00 - total_loss: 84674.4250\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 83200.0031 - regularization_loss: 0.0000e+00 - total_loss: 83200.0031\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 81703.1969 - regularization_loss: 0.0000e+00 - total_loss: 81703.1969\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 80145.1531 - regularization_loss: 0.0000e+00 - total_loss: 80145.1531\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 79205.1344 - regularization_loss: 0.0000e+00 - total_loss: 79205.1344\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 77956.2141 - regularization_loss: 0.0000e+00 - total_loss: 77956.2141\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 76989.8453 - regularization_loss: 0.0000e+00 - total_loss: 76989.8453\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 75901.2797 - regularization_loss: 0.0000e+00 - total_loss: 75901.2797\n",
            "Epoch 19/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 74944.0750 - regularization_loss: 0.0000e+00 - total_loss: 74944.0750\n",
            "Epoch 20/30\n",
            "5/5 [==============================] - 0s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 74232.5719 - regularization_loss: 0.0000e+00 - total_loss: 74232.5719\n",
            "Epoch 21/30\n",
            "5/5 [==============================] - 0s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 73392.0312 - regularization_loss: 0.0000e+00 - total_loss: 73392.0312\n",
            "Epoch 22/30\n",
            "5/5 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 72867.7859 - regularization_loss: 0.0000e+00 - total_loss: 72867.7859\n",
            "Epoch 23/30\n",
            "5/5 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 72557.9141 - regularization_loss: 0.0000e+00 - total_loss: 72557.9141\n",
            "Epoch 24/30\n",
            "5/5 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 71973.7516 - regularization_loss: 0.0000e+00 - total_loss: 71973.7516\n",
            "Epoch 25/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 71754.9547 - regularization_loss: 0.0000e+00 - total_loss: 71754.9547\n",
            "Epoch 26/30\n",
            "5/5 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 71246.2781 - regularization_loss: 0.0000e+00 - total_loss: 71246.2781\n",
            "Epoch 27/30\n",
            "5/5 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 71087.7125 - regularization_loss: 0.0000e+00 - total_loss: 71087.7125\n",
            "Epoch 28/30\n",
            "5/5 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 70856.4297 - regularization_loss: 0.0000e+00 - total_loss: 70856.4297\n",
            "Epoch 29/30\n",
            "5/5 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 70462.9656 - regularization_loss: 0.0000e+00 - total_loss: 70462.9656\n",
            "Epoch 30/30\n",
            "5/5 [==============================] - 0s 55ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 70545.6266 - regularization_loss: 0.0000e+00 - total_loss: 70545.6266\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efff6e0f580>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "model.fit(cached_train, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aeXfXYw4Xtk",
        "outputId": "f197caaf-0ed6-46eb-9e66-26ba3306f507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 3s 526ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0042 - factorized_top_k/top_5_categorical_accuracy: 0.0219 - factorized_top_k/top_10_categorical_accuracy: 0.0384 - factorized_top_k/top_50_categorical_accuracy: 0.1343 - factorized_top_k/top_100_categorical_accuracy: 0.2173 - loss: 25798.8757 - regularization_loss: 0.0000e+00 - total_loss: 25798.8757\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'factorized_top_k/top_1_categorical_accuracy': 0.004194697365164757,\n",
              " 'factorized_top_k/top_5_categorical_accuracy': 0.021923229098320007,\n",
              " 'factorized_top_k/top_10_categorical_accuracy': 0.038385435938835144,\n",
              " 'factorized_top_k/top_50_categorical_accuracy': 0.13430945575237274,\n",
              " 'factorized_top_k/top_100_categorical_accuracy': 0.21733281016349792,\n",
              " 'loss': 24549.404296875,\n",
              " 'regularization_loss': 0,\n",
              " 'total_loss': 24549.404296875}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}