{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "447R2K8kRY0Z"
      },
      "source": [
        "## Imports\n",
        "\n",
        "First let's get our dependencies and imports out of the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM3uchF8deZL"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfpLMl1tc4OE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://files.grouplens.org/datasets/movielens/ml-1m.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjoGK4LiDqN3",
        "outputId": "cb3dee06-9b3f-4094-bdb2-f55069dc55e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘ml-1m.zip’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o \"ml-1m.zip\"  -d  \"/content\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7b-mSPQFiiS",
        "outputId": "062f83e7-1039-4576-fe1f-3ce95d6c6665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ml-1m.zip\n",
            "  inflating: /content/ml-1m/movies.dat  \n",
            "  inflating: /content/ml-1m/ratings.dat  \n",
            "  inflating: /content/ml-1m/README   \n",
            "  inflating: /content/ml-1m/users.dat  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "ratings_df = pd.read_csv('ml-1m/ratings.dat', sep='::', header=0, skipinitialspace=True, encoding=\"unicode_escape\")\n",
        "ratings_df.dropna(inplace=True)\n",
        "movies_df = pd.read_csv('ml-1m/movies.dat', sep='::', header=0, skipinitialspace=True, encoding=\"unicode_escape\")\n",
        "movies_df.dropna(inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfHLWUCOGkMt",
        "outputId": "71c90cae-f622-4501-fa14-e036a00a9dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ratings_df.columns =[ 'userID', 'itemID', 'rating','timestamp']\n",
        "movies_df.columns =[ 'itemID', 'name','category']\n",
        "movies_df['itemID']=movies_df['itemID'].astype(str)\n",
        "items_df=movies_df['itemID']"
      ],
      "metadata": {
        "id": "XG_U_Y_hG6bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Movie Lens"
      ],
      "metadata": {
        "id": "ChfSMZqAI0i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocessing"
      ],
      "metadata": {
        "id": "Oqb-57AF_NiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#   you may not use this file except in compliance with the License.\n",
        "#   You may obtain a copy of the License at\n",
        "#\n",
        "#         http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#   Unless required by applicable law or agreed to in writing, software\n",
        "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#   See the License for the specific language governing permissions and\n",
        "#   limitations under the License.\n",
        "\"\"\"Prepare TF.Examples for on-device recommendation model.\n",
        "\n",
        "Following functions are included: 1) downloading raw data 2) processing to user\n",
        "activity sequence and splitting to train/test data 3) convert to TF.Examples\n",
        "and write in output location.\n",
        "\n",
        "More information about the itemlens dataset can be found here:\n",
        "https://grouplens.org/datasets/itemlens/\n",
        "\"\"\"\n",
        "\n",
        "import collections\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# Permalinks to download itemlens data.\n",
        "\n",
        "RATINGS_DATA_COLUMNS = [\"userID\",'itemID', 'rating','timestamp']\n",
        "itemS_DATA_COLUMNS = [\"itemID\"]\n",
        "OUTPUT_TRAINING_DATA_FILENAME = \"train.tfrecord\"\n",
        "OUTPUT_TESTING_DATA_FILENAME = \"test.tfrecord\"\n",
        "OUTPUT_ITEM_VOCAB_FILENAME = \"item_vocab.json\"\n",
        "PAD_ITEM_ID = 0\n",
        "PAD_RATING = 0.0\n",
        "UNKNOWN_STR = \"UNK\"\n",
        "VOCAB_ITEM_ID_INDEX = 0\n",
        "VOCAB_COUNT_INDEX = 0\n",
        "\n",
        "\n",
        "class ItemInfo(\n",
        "    collections.namedtuple(\n",
        "        \"ItemInfo\", [\"item_id\", \"timestamp\", \"rating\"])):\n",
        "  \"\"\"Data holder of basic information of a item.\"\"\"\n",
        "  __slots__ = ()\n",
        "\n",
        "  def __new__(cls,\n",
        "              item_id=0,\n",
        "              timestamp=0,\n",
        "              rating=0,\n",
        "             ):\n",
        "    return super(ItemInfo, cls).__new__(cls, item_id, timestamp, rating,\n",
        "                                        )\n",
        "\n",
        "\n",
        "def convert_to_timelines(ratings_df):\n",
        "  \"\"\"Convert ratings data to user.\"\"\"\n",
        "  timelines = collections.defaultdict(list)\n",
        "  item_counts = collections.Counter()\n",
        "  for user_id, item_id, rating, timestamp in ratings_df.values:\n",
        "    timelines[user_id].append(\n",
        "        ItemInfo(item_id=item_id, timestamp=int(timestamp), rating=rating))\n",
        "    item_counts[item_id] += 1\n",
        "  # Sort per-user timeline by timestamp\n",
        "  for (user_id, context) in timelines.items():\n",
        "    context.sort(key=lambda x: x.timestamp)\n",
        "    timelines[user_id] = context\n",
        "  return timelines, item_counts\n",
        "\n",
        "\n",
        "def generate_items_dict(items_df):\n",
        "  \"\"\"Generates items dictionary from items dataframe.\"\"\"\n",
        "  item_dict = {\n",
        "      item_id: ItemInfo(item_id=item_id)\n",
        "      for item_id in items_df.values\n",
        "  }\n",
        "  item_dict[0] = ItemInfo()\n",
        "  return item_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_examples_from_single_timeline(timeline,\n",
        "                                           items_dict,\n",
        "                                           max_context_len=100,\n",
        "                                           max_context_item_genre_len=320):\n",
        "  \"\"\"Generate TF examples from a single user timeline.\n",
        "\n",
        "  Generate TF examples from a single user timeline. Timeline with length less\n",
        "  than minimum timeline length will be skipped. And if context user history\n",
        "  length is shorter than max_context_len, features will be padded with default\n",
        "  values.\n",
        "\n",
        "  Args:\n",
        "    timeline: The timeline to generate TF examples from.\n",
        "    items_dict: Dictionary of all itemInfos.\n",
        "    max_context_len: The maximum length of the context. If the context history\n",
        "      length is less than max_context_length, features will be padded with\n",
        "      default values.\n",
        "    max_context_item_genre_len: The length of item genre feature.\n",
        "\n",
        "  Returns:\n",
        "    examples: Generated examples from this single timeline.\n",
        "  \"\"\"\n",
        "  examples = []\n",
        "  for label_idx in range(1, len(timeline)):\n",
        "    start_idx = max(0, label_idx - max_context_len)\n",
        "    context = timeline[start_idx:label_idx]\n",
        "    # Pad context with out-of-vocab item id 0.\n",
        "    while len(context) < max_context_len:\n",
        "      context.append(ItemInfo())\n",
        "    label_item_id = int(timeline[label_idx].item_id)\n",
        "    context_item_id = [int(ids.item_id) for ids in context]\n",
        "   \n",
        "    feature = {\n",
        "        \"context_item_id\":\n",
        "            tf.train.Feature(\n",
        "                int64_list=tf.train.Int64List(value=context_item_id)),\n",
        "       \n",
        "        \"label_item_id\":\n",
        "            tf.train.Feature(\n",
        "                int64_list=tf.train.Int64List(value=[label_item_id]))\n",
        "    }\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    examples.append(tf_example)\n",
        "\n",
        "  return examples\n",
        "\n",
        "\n",
        "def generate_examples_from_timelines(timelines,\n",
        "                                     items_df,\n",
        "                                     min_timeline_len=3,\n",
        "                                     max_context_len=100,\n",
        "                                     max_context_item_genre_len=320,\n",
        "                                     train_data_fraction=0.9,\n",
        "                                     random_seed=None,\n",
        "                                     shuffle=True):\n",
        "  \"\"\"Convert user timelines to tf examples.\n",
        "\n",
        "  Convert user timelines to tf examples by adding all possible context-label\n",
        "  pairs in the examples pool.\n",
        "\n",
        "  Args:\n",
        "    timelines: The user timelines to process.\n",
        "    items_df: The dataframe of all items.\n",
        "    min_timeline_len: The minimum length of timeline. If the timeline length is\n",
        "      less than min_timeline_len, empty examples list will be returned.\n",
        "    max_context_len: The maximum length of the context. If the context history\n",
        "      length is less than max_context_length, features will be padded with\n",
        "      default values.\n",
        "    max_context_item_genre_len: The length of item genre feature.\n",
        "    train_data_fraction: Fraction of training data.\n",
        "    random_seed: Seed for randomization.\n",
        "    shuffle: Whether to shuffle the examples before splitting train and test\n",
        "      data.\n",
        "\n",
        "  Returns:\n",
        "    train_examples: TF example list for training.\n",
        "    test_examples: TF example list for testing.\n",
        "  \"\"\"\n",
        "  examples = []\n",
        "  items_dict = generate_items_dict(items_df)\n",
        "  progress_bar = tf.keras.utils.Progbar(len(timelines))\n",
        "\n",
        "  for timeline in timelines.values():\n",
        "    if len(timeline) < min_timeline_len:\n",
        "      progress_bar.add(1)\n",
        "      continue\n",
        "    single_timeline_examples = generate_examples_from_single_timeline(\n",
        "        timeline=timeline,\n",
        "        items_dict=items_dict,\n",
        "        max_context_len=max_context_len,\n",
        "        max_context_item_genre_len=max_context_item_genre_len)\n",
        "    examples.extend(single_timeline_examples)\n",
        "   \n",
        "    progress_bar.add(1)\n",
        "  # Split the examples into train, test sets.\n",
        "  if shuffle:\n",
        "    random.seed(random_seed)\n",
        "    random.shuffle(examples)\n",
        "  last_train_index = round(len(examples) * train_data_fraction)\n",
        "\n",
        "  train_examples = examples[:last_train_index]\n",
        "  test_examples = examples[last_train_index:]\n",
        "  return train_examples, test_examples\n",
        "\n",
        "\n",
        "def generate_item_feature_vocabs(items_df, item_counts):\n",
        "  \"\"\"Generate vocabularies for item features.\n",
        "\n",
        "  Generate vocabularies for item features (item_id, genre, year), sorted by\n",
        "  usage count. Vocab id 0 will be reserved for default padding value.\n",
        "\n",
        "  Args:\n",
        "    items_df: Dataframe for items.\n",
        "    item_counts: Counts that each item is rated.\n",
        "\n",
        "  Returns:\n",
        "    item_id_vocab: List of all item ids paired with item usage count, and\n",
        "      sorted by counts.\n",
        "    item_genre_vocab: List of all item genres, sorted by genre usage counts.\n",
        "    item_year_vocab: List of all item years, sorted by year usage counts.\n",
        "  \"\"\"\n",
        "  item_vocab = []\n",
        "\n",
        "  for item_id in items_df.values:\n",
        "    count = item_counts.get(item_id) or 0\n",
        "    item_vocab.append([item_id, count])\n",
        "  \n",
        "  item_vocab.sort(key=lambda x: x[0], reverse=True)  # by count\n",
        "  \n",
        "  return item_vocab\n",
        "\n",
        "\n",
        "def write_tfrecords(tf_examples, filename):\n",
        "  \"\"\"Writes tf examples to tfrecord file, and returns the count.\"\"\"\n",
        "  with tf.io.TFRecordWriter(filename) as file_writer:\n",
        "    length = len(tf_examples)\n",
        "    progress_bar = tf.keras.utils.Progbar(length)\n",
        "    for example in tf_examples:\n",
        "      file_writer.write(example.SerializeToString())\n",
        "      progress_bar.add(1)\n",
        "    return length\n",
        "\n",
        "\n",
        "def write_vocab_json(vocab, filename):\n",
        "  \"\"\"Write generated item vocabulary to specified file.\"\"\"\n",
        "  with open(filename, \"w\", encoding=\"utf-8\") as jsonfile:\n",
        "    json.dump(vocab, jsonfile, indent=2)\n",
        "\n",
        "\n",
        "def write_vocab_txt(vocab, filename):\n",
        "  with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in vocab:\n",
        "      f.write(str(item) + \"\\n\")\n",
        "\n",
        "\n",
        "def generate_datasets(extracted_data_dir,\n",
        "                      output_dir,\n",
        "                      min_timeline_length,\n",
        "                      max_context_length,\n",
        "                      max_context_item_genre_length,\n",
        "                      min_rating=None,\n",
        "                      build_vocabs=True,\n",
        "                      train_data_fraction=0.9,\n",
        "                      train_filename=OUTPUT_TRAINING_DATA_FILENAME,\n",
        "                      test_filename=OUTPUT_TESTING_DATA_FILENAME,\n",
        "                      vocab_filename=OUTPUT_ITEM_VOCAB_FILENAME,\n",
        "                      ):\n",
        "  \"\"\"Generates train and test datasets as TFRecord, and returns stats.\"\"\"\n",
        "\n",
        "  logging.info(\"Generating item rating user timelines.\")\n",
        "  timelines, item_counts = convert_to_timelines(ratings_df)\n",
        "  logging.info(\"Generating train and test examples.\")\n",
        "  train_examples, test_examples = generate_examples_from_timelines(\n",
        "      timelines=timelines,\n",
        "      items_df=items_df,\n",
        "      min_timeline_len=min_timeline_length,\n",
        "      max_context_len=max_context_length,\n",
        "      max_context_item_genre_len=max_context_item_genre_length,\n",
        "      train_data_fraction=train_data_fraction)\n",
        "\n",
        "  if not tf.io.gfile.exists(output_dir):\n",
        "    tf.io.gfile.makedirs(output_dir)\n",
        "  logging.info(\"Writing generated training examples.\")\n",
        "  train_file = os.path.join(output_dir, train_filename)\n",
        "  train_size = write_tfrecords(tf_examples=train_examples, filename=train_file)\n",
        "  logging.info(\"Writing generated testing examples.\")\n",
        "  test_file = os.path.join(output_dir, test_filename)\n",
        "  test_size = write_tfrecords(tf_examples=test_examples, filename=test_file)\n",
        "  stats = {\n",
        "      \"train_size\": train_size,\n",
        "      \"test_size\": test_size,\n",
        "      \"train_file\": train_file,\n",
        "      \"test_file\": test_file,\n",
        "  }\n",
        "\n",
        "  if build_vocabs:\n",
        "    item_vocab = (\n",
        "        generate_item_feature_vocabs(\n",
        "            items_df=items_df, item_counts=item_counts))\n",
        "    vocab_file = os.path.join(output_dir, vocab_filename)\n",
        "    write_vocab_json(item_vocab, filename=vocab_file)\n",
        "    stats.update({\n",
        "        \"vocab_size\": len(item_vocab),\n",
        "        \"vocab_file\": vocab_file,\n",
        "        \"vocab_max_id\": max([arr[VOCAB_ITEM_ID_INDEX] for arr in item_vocab])\n",
        "    })\n",
        "\n",
        "    \n",
        "\n",
        "  return stats\n",
        "\n",
        "\n",
        "\n",
        "stats = generate_datasets(\n",
        "      extracted_data_dir=\"ml-1m\",\n",
        "      output_dir=\"data/processing\",\n",
        "      min_timeline_length=3,\n",
        "      max_context_length=10,\n",
        "      max_context_item_genre_length=4,\n",
        "      min_rating=0,\n",
        "      build_vocabs=True,\n",
        "      train_data_fraction=.8,\n",
        "  )\n",
        "print(stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbNqAEsmztKe",
        "outputId": "17310ae1-8200-4a35-ad48-f329132118b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6040/6040 [==============================] - 23s 4ms/step\n",
            "795334/795334 [==============================] - 10s 12us/step\n",
            "198834/198834 [==============================] - 2s 12us/step\n",
            "{'train_size': 795334, 'test_size': 198834, 'train_file': 'data/processing/train.tfrecord', 'test_file': 'data/processing/test.tfrecord', 'vocab_size': 3882, 'vocab_file': 'data/processing/item_vocab.json', 'vocab_max_id': '999'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_filename = \"./data/processing/train.tfrecord\"\n",
        "train = tf.data.TFRecordDataset(train_filename)\n",
        "\n",
        "test_filename = \"./data/processing/test.tfrecord\"\n",
        "test = tf.data.TFRecordDataset(test_filename)\n"
      ],
      "metadata": {
        "id": "dYVtG8cZThAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_description = {\n",
        "    'context_item_id': tf.io.FixedLenFeature([10], tf.int64, default_value=np.repeat(0, 10)),\n",
        "    'label_item_id': tf.io.FixedLenFeature([1], tf.int64, default_value=0),\n",
        "}\n"
      ],
      "metadata": {
        "id": "_9GT4jw5ThFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def _parse_function(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "train_ds = train.map(_parse_function).map(lambda x: {\n",
        "    \"context_item_id\": tf.strings.as_string(x[\"context_item_id\"]),\n",
        "    \"label_item_id\": tf.strings.as_string(x[\"label_item_id\"])\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "YuXhbXctXlhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for x in train_ds.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj53oWV5ZrQl",
        "outputId": "6d4b224b-9447-49b3-9931-ff714e7289f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'context_item_id': array([b'2003', b'1339', b'2657', b'2746', b'2120', b'2617', b'2004',\n",
            "       b'2367', b'2717', b'1690'], dtype=object),\n",
            " 'label_item_id': array([b'1388'], dtype=object)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ2aLaCndSYP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "test_ds = test.map(_parse_function).map(lambda x: {\n",
        "    \"context_item_id\": tf.strings.as_string(x[\"context_item_id\"]),\n",
        "    \"label_item_id\": tf.strings.as_string(x[\"label_item_id\"])\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szgmv8fvc7Tj"
      },
      "outputs": [],
      "source": [
        "# movies = tfds.load(\"movielens/1m-movies\", split='train')\n",
        "# movies = movies.map(lambda x: x[\"movie_id\"])\n",
        "# movie_ids = movies.batch(1_000)\n",
        "# unique_movie_ids = np.unique(np.concatenate(list(movie_ids)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df['itemID']=ratings_df['itemID'].astype(str)\n",
        "items_dict = ratings_df[['itemID']].drop_duplicates()\n",
        "\n",
        "movies= tf.data.Dataset.from_tensor_slices(dict(items_dict))\n",
        "movies = movies.map(lambda x: x['itemID'])\n",
        "items_list =movies.batch(1_000)\n",
        "unique_movie_ids = unique_items = np.unique(np.concatenate(list(items_list),axis =0))\n"
      ],
      "metadata": {
        "id": "H6KSp_hN3c9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxFmdiVxT8j_"
      },
      "source": [
        "## Implementing a sequential model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP3OS_HPeefT"
      },
      "outputs": [],
      "source": [
        "embedding_dimension = 32\n",
        "\n",
        "query_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_movie_ids, mask_token=None),\n",
        "    tf.keras.layers.Embedding(len(unique_movie_ids) + 1, embedding_dimension), \n",
        "    tf.keras.layers.GRU(embedding_dimension),\n",
        "])\n",
        "\n",
        "candidate_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_movie_ids, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_movie_ids) + 1, embedding_dimension)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ich7YzXXAWq"
      },
      "source": [
        "The metrics, task and full model are defined similar to the basic retrieval model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KDYuhG9u9cq"
      },
      "outputs": [],
      "source": [
        "metrics = tfrs.metrics.FactorizedTopK(\n",
        "  candidates=movies.batch(128).map(candidate_model)\n",
        ")\n",
        "\n",
        "task = tfrs.tasks.Retrieval(\n",
        "  metrics=metrics\n",
        ")\n",
        "\n",
        "class Model(tfrs.Model):\n",
        "\n",
        "    def __init__(self, query_model, candidate_model):\n",
        "        super().__init__()\n",
        "        self._query_model = query_model\n",
        "        self._candidate_model = candidate_model\n",
        "\n",
        "        self._task = task\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "      \n",
        "        watch_history = features[\"context_item_id\"]\n",
        "        watch_next_label = features[\"label_item_id\"]\n",
        "\n",
        "        query_embedding = self._query_model(watch_history)       \n",
        "        candidate_embedding = self._candidate_model(watch_next_label)\n",
        "        \n",
        "        return self._task(query_embedding, candidate_embedding, compute_metrics=not training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j22_GTTeXa_T"
      },
      "source": [
        "## Fitting and evaluating\n",
        "\n",
        "We can now compile, train and evaluate our sequential retrieval model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c2wAqPVXYgz"
      },
      "outputs": [],
      "source": [
        "model = Model(query_model, candidate_model)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wto55LJWuj0"
      },
      "outputs": [],
      "source": [
        "cached_train = train_ds.shuffle(10_000).batch(12800).cache()\n",
        "cached_test = test_ds.batch(2560).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcXH5aDcWyJJ",
        "outputId": "62caecf6-b667-4ac8-d7af-53a7e186cbab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "64/64 [==============================] - 42s 626ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 106193.6887 - regularization_loss: 0.0000e+00 - total_loss: 106193.6887\n",
            "Epoch 2/3\n",
            "64/64 [==============================] - 4s 67ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 98873.9681 - regularization_loss: 0.0000e+00 - total_loss: 98873.9681\n",
            "Epoch 3/3\n",
            "64/64 [==============================] - 4s 67ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 96926.8686 - regularization_loss: 0.0000e+00 - total_loss: 96926.8686\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5b0cc22a00>"
            ]
          },
          "metadata": {},
          "execution_count": 602
        }
      ],
      "source": [
        "model.fit(cached_train, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aeXfXYw4Xtk",
        "outputId": "3fde401f-60cf-46a3-c491-0697ff40f81a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78/78 [==============================] - 68s 862ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0149 - factorized_top_k/top_5_categorical_accuracy: 0.0781 - factorized_top_k/top_10_categorical_accuracy: 0.1354 - factorized_top_k/top_50_categorical_accuracy: 0.3705 - factorized_top_k/top_100_categorical_accuracy: 0.4995 - loss: 15700.2516 - regularization_loss: 0.0000e+00 - total_loss: 15700.2516\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'factorized_top_k/top_1_categorical_accuracy': 0.01490187831223011,\n",
              " 'factorized_top_k/top_5_categorical_accuracy': 0.07811541110277176,\n",
              " 'factorized_top_k/top_10_categorical_accuracy': 0.13544967770576477,\n",
              " 'factorized_top_k/top_50_categorical_accuracy': 0.37050503492355347,\n",
              " 'factorized_top_k/top_100_categorical_accuracy': 0.4994870126247406,\n",
              " 'loss': 9968.255859375,\n",
              " 'regularization_loss': 0,\n",
              " 'total_loss': 9968.255859375}"
            ]
          },
          "metadata": {},
          "execution_count": 603
        }
      ],
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny2gMW3MdMKW"
      },
      "source": [
        "Reference:\n",
        "Tensorflow recommenders "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}